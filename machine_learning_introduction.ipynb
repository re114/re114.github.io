{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-learning-introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAGuLD9Xvnd"
      },
      "source": [
        "##Approaches to machine learning\n",
        "###Supervised Learning  \n",
        "In supervised learning we take a set of data and a set of labels that map to the data, xi -> yi, the dataset is split to provide a training set and a test set, the model is trained on the data and labels, and tested on the test data, the aim is to reduce the error. Supervised learning is typically used for Classification or Regression problems.\n",
        "A Regression problem involves predicting a numerical label, an example of regression would be projections of new coronavirus cases based on historic data, or data from other countries such as provided by worldmeter (“United Kingdom Coronavirus cases,” n.d.).\n",
        "A Classification problem aims to identify a class label, an example of a classification problem would be the digit identification we looked at with the MNIST database (LeCunn et al., n.d.), which takes data and label pairs (x,y) x is data y is label, with the goal of learning a function to map x-> y.\n",
        "The MNIST dataset consists of 60000 labelled images of the digits 0-9. (LeCunn et al., n.d.) So a supervised model might take 40000 images and labels as the training set and run the model on these. Once the model is trained it is run with the test data.\n",
        "A good model will give similar test results to training results. A model that has been too closely training is said to have been overfitted to the training data which. This highlights the importance of splitting the data into at least training and test data (better still, training, verification and testing data) sets to prevent unseen correlations from swaying the model.  \n",
        "##TODO add rationale for use of SL\n",
        "  \n",
        ">“Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hypothesis we give it a test set of examples that are distinct from the training set.”  \n",
        "(Norvig & Stuart, 2010).\n",
        "\n",
        "###Unsupervised learning.  \n",
        "Unsupervised learning does not require labelled data, and instead actively looks for correlations within the data to learn the underlying structure - \"is this thing like another thing?\"\n",
        "Unsupervised learning is typically used for clustering or density estimation problems.\n",
        "An example of a problem addressed by clustering is spam filtering, which can use K-Means clustering to at the email header and content and create groups, or clusters to identify problem emails.(“(28) Lecture 13.2 — Clustering | KMeans Algorithm — [ Machine Learning | Andrew Ng ] - YouTube,” n.d.)  \n",
        "##TODO add rationale for value of USL\n",
        "\n",
        ">“The most common unsupervised learning task is clustering detecting potentially useful clusters of input examples. For example, a taxi agent might gradually develop a concept of “good traffic days” and “bad traffic days” without ever being given labelled examples of each by a teacher.”  \n",
        "(Norvig & Stuart, 2010).  \n",
        "\n",
        "###Reinforcement learning  \n",
        "Reinforcement learning places the machine learning agent in an environment and lets it learn using feedback and success against a success criterion. It takes data as state action pairs and sets goals based on maximum future rewards over many time steps.\n",
        ">“Reinforcement learning is learning what to do — how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.”  \n",
        "(Sutton & Barto, 2018). \n",
        "\n",
        "An example of reinforcement learning would be the (very popular with Computer Science students) work on AI systems learning to play Video games (Shao, Tang, Zhu, Li, & Zhao, 2018).  \n",
        "\n",
        "However the difficulty of configuring an effective Reinforcement Learning model is substantial, even for exteremely competent researcher like Andrew Karpathy.  \n",
        ">\t\n",
        "karpathy on Hacker News Jan 30, 2017 | parent | favorite | on: Outrageously Large Neural Networks: The Sparsely-G...\n",
        "If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.\n",
        "Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.\n",
        "SL wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.\n",
        "Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of \"designing neural networks\".\n",
        "\n",
        "\n",
        "##TODO add rationale for not using RL\n",
        "Reinforecement Learning is hard because of the reward sparsity, and the need to incremental which can lead to deception. The models do not generalise at all well, and the sample efficiency is dreadful.[TODO add ref to Alex Irpan Deep reinforcement learning doesn't work yet\"]\n",
        "For these reasons I will not be using deep reinforcement learning.\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e558IG-wgUuu"
      },
      "source": [
        "\n",
        "###Transfer Learning\n",
        "Transfer learning allows parts of a pre trained neural network to be re-used, saving time and money. The simplest route is to take a prexisting trained model. Freeze some of the layers to set their trained weights and add additional more specific layers.  \n",
        "\n",
        "This reduces the overhead for basic feature extraction, re-using those convolutional layers close to the input, and concentrates on aspects particular to the use case training the fully connected layers.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOYrT7OFYX4l"
      },
      "source": [
        "###Neural Networks  \n",
        "It's interesting to consider that the original work undertaken by McCulloch and Pitts (Mcculloch & Pitts, 1990) on neural brain structures as logic gates informed and inspired Von-Neumann's architecture (Ohta, 2015) and his view of the computer as a brain. The paradigm shifted and we began to view the brain as a type of computer, and as we came round to neural networks the analogy switched back once more (Cobb, 2020).  \n",
        "Early work on Neural Networks was carried out by Frank Rosenblatt, who described the structure of the perceptron (Rosenblatt, 1958).  \n",
        "Rosenblatt may have overhyped his findings, and fed a media circus instead of managing expectations (Boden, 2006).  \n",
        "  \n",
        "![Rosenblatts Perceptron](https://raw.githubusercontent.com/re114/robotdraws/main/img/Rosenblatt-perceptron.png)\n",
        "\n",
        "Minksy at MIT published a damning mathematical analysis of Rosenblatt's work (Minsky, 1961) which many cite as precipitating the first AI winter (Boden, 2006) (Norvig & Stuart, 2010). The paper suggested that the perceptron was a dead end for AI as it could not internally represent the things it was learning (Cobb, 2020) and it was not until the adoption of backpropagation that the approach became ascendant again (Y. LeCun et al., 1989).  \n",
        "  \n",
        "###The structure of Neural Networks  \n",
        "Bengio quotes Hinton: \"You have relatively simple processing elements that are very loosely models of neurons. They have connections coming in , each connection has a weight on it, and that weight can be changed through learning” (LeCunn, Bottou, & Haffner, 1998).  \n",
        "  \n",
        "Chollet outlines the processes in the operation of a Neural Network (François Chollet, 2019).  \n",
        "• define  \n",
        "• fit  \n",
        "• predict  \n",
        "• and evaluate  \n",
        "• Initialise weights randomly - or by some insight into the relative importance of hyperparameters.  \n",
        "• loop till convergence  \n",
        "• compute gradient - (derivative)  \n",
        "• update weights  \n",
        "• return weights  \n",
        "In practical terms the aim is to minimise the error. We can describe the error as the absolute difference between the prediction and the results.  \n",
        "\n",
        "```\n",
        "error = ((input * weight) - goal.pred) **2\n",
        "```\n",
        "\n",
        "The weight determines the significance of the input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHx0iy1FzJvz"
      },
      "source": [
        "This script trains a basic neural network on a set of labelled data to classify images as either cats or dogs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt9n6cE_kwXU",
        "outputId": "20455fcb-f79b-4cfc-eac8-9683b30bdd05"
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "[[ 0.5176658  -0.13445848  0.02820197  0.04942533 -0.6842776  -0.26025444\n",
            "   0.27671477 -0.6232597  -0.06118366 -0.44626555]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G1WLFB_wmko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1a3751-a91c-48fc-c262-849a6c6bc564"
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17962019, 0.09357098, 0.11009909, 0.11246073, 0.05399552,\n",
              "        0.0825104 , 0.14115994, 0.05739281, 0.10068483, 0.06850545]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0S9cf7wwss8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4f7c4c-92ce-4921-8499-f7e51545cd60"
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn(y_train[:1], predictions).numpy()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v22sNanHw0Bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7331ef7-f573-4a74-aab3-7ae15d321792"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4892 - accuracy: 0.8557\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1582 - accuracy: 0.9536\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1138 - accuracy: 0.9657\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0923 - accuracy: 0.9721\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0743 - accuracy: 0.9774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9dc2eb0a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY8iXFdfxBR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c251c86-b392-4040-9c07-26868eb56456"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 0s - loss: 0.0764 - accuracy: 0.9766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07643616944551468, 0.9765999913215637]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DAG0aEpxGJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63957c80-3bd9-40b4-d800-fa09d2c49c7d"
      },
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])\n",
        "\n",
        "probability_model(x_test[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[2.3982443e-09, 6.4169414e-10, 1.0888503e-05, 3.0259458e-05,\n",
              "        3.3600733e-11, 1.7275735e-07, 4.4016707e-15, 9.9995506e-01,\n",
              "        2.0105460e-07, 3.4885350e-06],\n",
              "       [2.0737152e-09, 2.4616753e-03, 9.9752265e-01, 1.4214254e-05,\n",
              "        5.7037852e-15, 3.9163100e-07, 7.7725552e-08, 6.7630531e-13,\n",
              "        9.1622178e-07, 2.7283492e-13],\n",
              "       [2.7055131e-07, 9.9863869e-01, 2.0268162e-04, 5.1933890e-05,\n",
              "        1.9910194e-04, 1.2790084e-05, 1.4361428e-04, 6.0762779e-04,\n",
              "        1.4009200e-04, 3.2012922e-06],\n",
              "       [9.9954587e-01, 7.9242646e-10, 5.4218904e-06, 1.5822366e-07,\n",
              "        4.7550991e-05, 2.0334862e-06, 2.9748009e-04, 8.9524649e-05,\n",
              "        1.1174333e-07, 1.1786353e-05],\n",
              "       [3.9327824e-06, 1.9700616e-08, 1.1182532e-05, 2.0912091e-07,\n",
              "        9.8954433e-01, 1.0499693e-06, 1.2581231e-05, 8.8090062e-05,\n",
              "        2.3666976e-06, 1.0336231e-02]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU5oj5dg4dS"
      },
      "source": [
        "We can gauge how accurate our predictions are by comparing the prediction with a known result or label.  \n",
        "In order to benefit from multiple layers, we need to add an activation function, without this the layers could practically be collapsed into a single layer (H. Li, Ouyang, & Wang, 2016).  \n",
        "The activation function maps back to the neural structure of biological systems where synaptic inputs are expressed or repressed (Hawkins, 2005) to determine their activation.\n",
        "We can use a range of activation functions, such as RELU - rectified linear units which only activates if the input is above zero.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbMrzVGV1VtY"
      },
      "source": [
        "###Backpropagation\n",
        "Whilst backpropagation was discovered in the 1970s, it was neglected until a 1986 Nature paper co-authored by Geoffrey Hinton:\n",
        ">\"At its essence backpropagation is just a clever application of the chain rule.\"  \n",
        "(Rumelhart, Hinton, & Williams, 1986)  \n",
        "\n",
        ">\"The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units\"  \n",
        "\n",
        "\n",
        "Echoing our previous note on the parallel and complementary development of AI and cognitive neuroscience, Hinton has recently co-authored a paper \"Backpropagation and the brain\" (Lillicrap, Santoro, Marris, Akerman, & Hinton, 2020) which argues that \"feedback connections may induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.\"  \n",
        "\n",
        "Backpropagation is an expression for the partial derivative of the cost function with respect to any weight or bias in the network.  \n",
        "The expression tells the network how quickly the cost changes when the weights and biases are changed (Nielsen, 2015).  \n",
        "More pragmatically LeCunn describes backpropagation as: “a very popular neural network learning algorithm because it is conceptually simple, computationally efficient, and because it often works.”  \n",
        "It is worth noting that LeCunn continues to identify the lack of precision in determining initial conditions at the heart of backpropagation, a theme that emerges again when deciding policies on dealing with missing data.  \n",
        "\n",
        ">“However, getting it to work well, and sometimes to work at all, can seem more of an art than a science. Designing and training a network using backprop requires making many seemingly arbitrary choices such as the number and types of nodes, layers, learning rates, training and test sets, and so forth. These choices can be critical, yet there is no fool proof recipe for deciding them because they are largely problem and data dependent. However, there are heuristics and some underlying theory that can help guide a practitioner to make better choices.”  \n",
        "(Y. A. LeCun, Bottou, Orr, & Müller, 2012).  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo8GUn3D2EKR"
      },
      "source": [
        "# code example here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXlEQ3yC2Gzt"
      },
      "source": [
        "##Recurrent Neural Nets\n",
        ">“A Recurrent Neural Network is a type of neural network that contains loops, allowing information to be stored within the network. In short, Recurrent Neural Networks use their reasoning from previous experiences to inform the upcoming events. Recurrent models are valuable in their ability to sequence vectors, which opens up the API to performing more complicated tasks.”  \n",
        "(“Recurrent Neural Network Definition | DeepAI,” n.d.)  \n",
        "\n",
        "Boucher quotes Trask: \"I feel like a significant percentage of Deep Learning breakthroughs ask the question ‘how can I re-use weights in multiple places?’ - Recurrent (LTSM) layers for multiple timesteps - Convolutional layers reuse layers in multiple locations - capsules reuse across orientation\" (Boucher, 2019)  \n",
        "\n",
        "Long Term Short memory networks are a subclass of recurrent neural network (Hochreiter & Schmidhuber, 1997) identified by Hochreiter and Schmidhuber in 1997 which allow data to be passed across layers in a neural network improving performance (Sherstinsky, 2018).  \n",
        "\n",
        ">“ A recurrent network whose inputs are not fixed but rather constitute an input sequence can be used to transform an input sequence into an output sequence while taking into account contextual information in a flexible way.”  \n",
        "(Bengio, Simard, & Frasconi, 1994).  \n",
        "\n",
        "Graves implementation of this was one of the spurs to this initial study:\n",
        ">“Unfortunately, the range of contextual information that standard RNNs can access is in practice quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections.\n",
        "This shortcoming ... referred to in the literature as the vanishing gradient problem ... Long Short-Term Memory (LSTM) is an RNN architecture specifically designed to address the vanishing gradient problem.”  \n",
        "(Graves et al., 2009)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgLZajt621iP"
      },
      "source": [
        "###Deep Learning Convolutional Neural Networks\n",
        "As highlighted by Boucher “Convolutional layers reuse layers in multiple locations” (Boucher, 2019).  \n",
        "Yan LeCunn outlined the architecture of the Convolutional Neural Network in 1998 (Yann LeCun, Bottou, Bengio, & Haffner, 1998) and (Bottou, Bengio, & Le Cun, 1997).  \n",
        "LeNet-5 consists of two sets of convolutional and average pooling layers, followed by a flattening convolutional layer, then two fully connected layers and finally a softmax classifier.  \n",
        "\n",
        "#TODO add Structure of LeNet5 source lecun.com  \n",
        "\n",
        "The CNN applies a sliding window like a filter, and every targeted pixel is multiplied by the value by the filter. The structure of the window determines what aspects of the input layer are amplified.\n",
        "\n",
        "#TODO add 2D convolution source Cambridge Coding Academy  \n",
        "\n",
        "e.g.\n",
        "This is an example a convolution to emphasise vertical lines:  \n",
        "\n",
        "-1 0 1  \n",
        "-2 0 2  \n",
        "-1 0 1  \n",
        "\n",
        "This is an example of a convolution to emphasise horizontal lines  \n",
        "\n",
        "-1 -2 -1  \n",
        " 0  0  0  \n",
        " 1  2  1  \n",
        "\n",
        "Pooling is the process by which we compress data representation of the image\n",
        "e.g.\n",
        "0 64  x  x  \n",
        "48 192 x x  \n",
        "xxxx  \n",
        "xxxx  \n",
        "\n",
        "To pool the top left quadrant we select the highest value:\n",
        "0 64  \n",
        "48 192  \n",
        "\n",
        "goes to 192  \n",
        "\n",
        "192 x   \n",
        "x   x\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB8UJOIA393a"
      },
      "source": [
        "# CNN example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-o2JzHVi632"
      },
      "source": [
        "###Model architecture  \n",
        "sequential vs functional  \n",
        "distributed  \n",
        "tpus and gpus  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RkRXT9SgacG"
      },
      "source": [
        "###Pre Existing models\n",
        "outline range of models rationale for   \n",
        "CITE papers with code - showing 1000 pre trained models https://paperswithcode.com/datasets?mod=images&page=1   \n",
        "\n",
        "\n",
        "###Model selection\n",
        "TODO - outline value of various models\n",
        "\n"
      ]
    }
  ]
}
