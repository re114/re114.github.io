{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-learning-introduction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxbHy1VIRr2hKijwMRWWgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/re114/re114.github.io/blob/main/machine_learning_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAGuLD9Xvnd"
      },
      "source": [
        "##Approaches to machine learning\n",
        "Supervised Learning  \n",
        "In supervised learning we take a set of data and a set of labels that map to the data, xi -> yi, the dataset is split to provide a training set and a test set, the model is trained on the data and labels, and tested on the test data, the aim is to reduce the error. Supervised learning is typically used for Classification or Regression problems.\n",
        "A Regression problem involves predicting a numerical label, an example of regression would be projections of new coronavirus cases based on historic data, or data from other countries such as provided by worldmeter (“United Kingdom Coronavirus cases,” n.d.).\n",
        "A Classification problem aims to identify a class label, an example of a classification problem would be the digit identification we looked at with the MNIST database (LeCunn et al., n.d.), which takes data and label pairs (x,y) x is data y is label, with the goal of learning a function to map x-> y.\n",
        "The MNIST dataset consists of 60000 labelled images of the digits 0-9. (LeCunn et al., n.d.) So a supervised model might take 40000 images and labels as the training set and run the model on these. Once the model is trained it is run with the test data.\n",
        "A good model will give similar test results to training results. A model that has been too closely training is said to have been overfitted to the training data which. This highlights the importance of splitting the data into at least training and test data (better still, training, verification and testing data) sets to prevent unseen correlations from swaying the model.\n",
        "  \n",
        "“Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hypothesis we give it a test set of examples that are distinct from the training set.”(Norvig & Stuart, 2010).\n",
        "\n",
        "Unsupervised learning.  \n",
        "Unsupervised learning does not require labelled data, and instead actively looks for correlations within the data to learn the underlying structure - \"is this thing like another thing?\"\n",
        "Unsupervised learning is typically used for clustering or density estimation problems.\n",
        "An example of a problem addressed by clustering is spam filtering, which can use K-Means clustering to at the email header and content and create groups, or clusters to identify problem emails.(“(28) Lecture 13.2 — Clustering | KMeans Algorithm — [ Machine Learning | Andrew Ng ] - YouTube,” n.d.)\n",
        "“The most common unsupervised learning task is clustering detecting potentially useful clusters of input examples. For example, a taxi agent might gradually develop a concept of “good traffic days” and “bad traffic days” without ever being given labelled examples of each by a teacher.”(Norvig & Stuart, 2010).  \n",
        "\n",
        "Reinforcement learning  \n",
        "Reinforcement learning places the machine learning agent in an environment and lets it learn using feedback and success against a success criterion. It takes data as state action pairs and sets goals based on maximum future rewards over many time steps.\n",
        "“Reinforcement learning is learning what to do — how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.”(Sutton & Barto, 2018) An example of reinforcement learning would be the (very popular with Computer Science students) work on AI systems learning to play Video games (Shao, Tang, Zhu, Li, & Zhao, 2018).  \n",
        "\n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOYrT7OFYX4l"
      },
      "source": [
        "Neural Networks  \n",
        "It's interesting to consider that the original work undertaken by McCulloch and Pitts (Mcculloch & Pitts, 1990) on neural brain structures as logic gates informed and inspired Von-Neumann's architecture (Ohta, 2015) and his view of the computer as a brain. The paradigm shifted and we began to view the brain as a type of computer, and as we came round to neural networks the analogy switched back once more (Cobb, 2020).  \n",
        "Early work on Neural Networks was carried out by Frank Rosenblatt, who described the structure of the perceptron (Rosenblatt, 1958). Rosenblatt may have overhyped his findings, and fed a media circus instead of managing expectations (Boden, 2006).  \n",
        "  \n",
        "\n",
        " Figure 9 An image of Rosenblatt's perceptron.\n",
        "Minksy at MIT published a damning mathematical analysis of Rosenblatt's work (Minsky, 1961) which many cite as precipitating the first AI winter (Boden, 2006) (Norvig & Stuart, 2010). The paper suggested that the perceptron was a dead end for AI as it could not internally represent the things it was learning (Cobb, 2020) and it was not until the adoption of backpropagation that the approach became ascendant again (Y. LeCun et al., 1989).  \n",
        "  \n",
        "The structure of Neural Networks  \n",
        "Bengio quotes Hinton: \"You have relatively simple processing elements that are very loosely models of neurons. They have connections coming in , each connection has a weight on it, and that weight can be changed through learning” (LeCunn, Bottou, & Haffner, 1998).  \n",
        "  \n",
        "Chollet outlines the processes in the operation of a Neural Network (François Chollet, 2019).  \n",
        "• define  \n",
        "• fit  \n",
        "• predict  \n",
        "• and evaluate  \n",
        "• Initialise weights randomly - or by some insight into the relative importance of hyperparameters.  \n",
        "• loop till convergence  \n",
        "• compute gradient - (derivative)  \n",
        "• update weights  \n",
        "• return weights  \n",
        "In practical terms the aim is to minimise the error. We can describe the error as the absolute difference between the prediction and the results.  \n",
        "\n",
        "```\n",
        "error = ((input * weight) - goal.pred) **2\n",
        "```\n",
        "\n",
        "The weight determines the significance of the input, for example in calculating the number of cases of COVID19 we might look at a datapoint such as age and give that factor a weighting.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT70AYV2eckm",
        "outputId": "8af31cdc-fbe2-448a-b9ee-93c2cf6e585d"
      },
      "source": [
        "weight = 0.7\n",
        "def neural_network(input, weight):\n",
        "    prediction = input * weight\n",
        "    return prediction\n",
        "\n",
        "age_of_person = [3, 13, 23, 33, 43, 53, 63, 73, 83, 93]\n",
        "\n",
        "for i in age_of_person:\n",
        "  pred = neural_network(i,weight)\n",
        "  print(pred)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0999999999999996\n",
            "9.1\n",
            "16.099999999999998\n",
            "23.099999999999998\n",
            "30.099999999999998\n",
            "37.099999999999994\n",
            "44.099999999999994\n",
            "51.099999999999994\n",
            "58.099999999999994\n",
            "65.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM9PNi7EedJ2"
      },
      "source": [
        "\n",
        "The example above gives a result of 2.1 where the input is the first element in the array, 9.1 for the second, and 65.1 for the last.  \n",
        "This shows the prediction rises with an increase in the input factor- age.\n",
        "We can test our predictions by comparing the predictions against labelled data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt9n6cE_kwXU",
        "outputId": "e916dc54-9571-4a44-c4a1-4561675b8046"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "Covid_prediction_model = Dense(units=1, input_shape=[1])\n",
        "model = Sequential([Covid_prediction_model])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "age = np.array([3, 13, 23, 43, 53, 63, 73, 83, 93], dtype=float)\n",
        "label = np.array([1, 1, 2, 30, 40, 60, 80, 85, 95], dtype=float)\n",
        "\n",
        "model.fit(age, label, epochs=5)\n",
        "\n",
        "print(model.predict([25]))\n",
        "print(\"Here is what I learned: {}\".format(Covid_prediction_model.get_weights()))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 0s 229ms/step - loss: 107.9813\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 13740.5908\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 59630652.0000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 260759388160.0000\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1140279188914176.0000\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f89b23e0560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[-9.640592e+08]]\n",
            "Here is what I learned: [array([[-38539550.]], dtype=float32), array([-570462.94], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU5oj5dg4dS"
      },
      "source": [
        "We can gauge how accurate our predictions are by comparing the prediction with a known result or label.  \n",
        "This approach is linear, and doesn't benefit from additional layers, which could practically be collapsed into a single layer (H. Li, Ouyang, & Wang, 2016).  \n",
        "In order to benefit from multiple layers, we need to add an activation function.  \n",
        "This maps back to the neural structure of biological systems where synaptic inputs are expressed or repressed (Hawkins, 2005) to determine their activation.\n",
        "We can use a range of activation functions, such as RELU - rectified linear units which only activates if the input is above zero.\n",
        "The result is to provide an inflexion point where a threshold must be crossed.  \n",
        "For example, if we train a simple network with the MNIST dataset by using activation functions we can benefit from additional hidden layers:   \n"
      ]
    }
  ]
}